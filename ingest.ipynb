{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e63300-9167-476a-83fd-195646552601",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Required packages\n",
    "#!pip install pymupdf\n",
    "#!pip install tqdm\n",
    "#!pip install sentence-transformers tensorflow\n",
    "#!pip install tf-keras\n",
    "#!pip install langchain-text-splitters\n",
    "\n",
    "# Embedding model https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "\n",
    "## optional, info only\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install flash-attn --no-build-isolation # failed because no GPU\n",
    "\n",
    "# Semantic chunk: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "# nltk\n",
    "#!pip install upgrade protobuf\n",
    "\n",
    "local_debug = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a8e477-640c-4cdc-b85f-f092a1c6921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pymupdf\n",
    "import re\n",
    "\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    para_end = \"<paraend>\"\n",
    "    word_cut = \"<wordcut>\"\n",
    "    words_to_clean = {\n",
    "        \"ﬁ \":\"fi\",\n",
    "        \"ﬂ \":\"fl\",\n",
    "        \".\\n\\n\\n\": para_end,\n",
    "        \".\\n\\n\": para_end,\n",
    "        \"\\n\\n\": para_end,\n",
    "        \":\\n\": para_end,\n",
    "        \".\\n\": para_end,\n",
    "        \"-\\n\": word_cut,\n",
    "        \"\\n\": \" \",\n",
    "        para_end : \".\\n\",\n",
    "        word_cut :\"\",\n",
    "    }\n",
    "\n",
    "    cleaned_text = text\n",
    "    for invalid_word, valid_word in words_to_clean.items():\n",
    "        cleaned_text = cleaned_text.replace(invalid_word, valid_word)        \n",
    "    \n",
    "    # Remove excessive spaces and replace with a single space    \n",
    "    cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict] :\n",
    "    try:        \n",
    "        doc = pymupdf.open(pdf_path)    \n",
    "        print(f\"Successfully opened '{pdf_path}'.\")\n",
    "        print(f\"Number of pages: {doc.page_count}\")    \n",
    "        header_height = 50 # Adjust as needed\n",
    "        footer_height = 50 # Adjust as needed\n",
    "        pages_and_texts = []\n",
    "        for page_number, page in tqdm(enumerate(doc)):\n",
    "            if page_number > 0 :\n",
    "                page_rect = page.rect\n",
    "                clip = pymupdf.Rect(\n",
    "                    page_rect.x0,\n",
    "                    page_rect.y0 + header_height,\n",
    "                    page_rect.x1,\n",
    "                    page_rect.y1 - footer_height\n",
    "                )\n",
    "                #get_text(option, *, clip=None, flags=None, textpage=None, sort=False, delimiters=None)                \n",
    "                text = page.get_text(clip=clip)\n",
    "                text = text_formatter(text)          \n",
    "                \n",
    "                if len(text) != 0 and page_number > 14 :\n",
    "                    pages_and_texts.append({\"page_number\": page_number-14,\n",
    "                                       \"page_char_count\": len(text),\n",
    "                                       \"page_word_count\": len(text.split(\" \")),\n",
    "                                       \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                       \"page_token_count\": len(text)/4, # 1 token =~4 char\n",
    "                                       \"text\": text })\n",
    "        \n",
    "        doc.close()\n",
    "        return pages_and_texts    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{pdf_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2868df5f-bb67-4bad-ac54-a214831c3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import os\n",
    "\n",
    "def extract_text_and_spell_check(pages_and_texts: list) -> list :\n",
    "    misspelled_words = []\n",
    "    raw_text = []    \n",
    "    misspell_file = \"misspelled_words_test.txt\"    \n",
    "    if os.path.exists(misspell_file):\n",
    "        os.remove(misspell_file)\n",
    "    \n",
    "    with open(misspell_file, 'w', encoding='utf-8') as file:\n",
    "        for item in pages_and_texts :   \n",
    "            raw_text.append(item[\"text\"])\n",
    "            cleaned_text = re.sub(r'[^\\w\\s]', '', item[\"text\"])\n",
    "            spell = SpellChecker()    \n",
    "            misspelled = spell.unknown(cleaned_text.split())    \n",
    "            for word in misspelled:\n",
    "                misspelled_words.append(word)\n",
    "                file.write(word + \"\\n\")\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91cb9c7-c217-4280-bfd6-97ea19e01496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anila\\ml\\mlenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anila\\ml\\mlenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def get_recursive_chunks(raw_text: list) -> list:    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=40,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try splitting by paragraphs, then newlines, then spaces, then characters\n",
    "    )    \n",
    "    docs = text_splitter.create_documents(raw_text)    \n",
    "    recursive_chunks = []\n",
    "    chunks_file = \"chunks_test.txt\"    \n",
    "    if os.path.exists(chunks_file):\n",
    "        os.remove(chunks_file)\n",
    "    \n",
    "    with open(chunks_file, 'w', encoding='utf-8') as file:\n",
    "        for i, doc in enumerate(docs):\n",
    "            recursive_chunks.append(doc.page_content)\n",
    "            if local_debug :                \n",
    "                file.write(doc.page_content + \"\\n\\n\")\n",
    "    return recursive_chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e017ae98-d132-40be-a241-e45e7f488b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SELECT id,SUBSTRING(ni.chunk, 1, 200) AS short_chunk, embedding as dimension FROM nutritionitems ni\n",
    "# ORDER BY embedding <=> '[5.99734336e-02,-1.30569497e-02]'\n",
    "# LIMIT 5;\n",
    "# drop table nutritionitems;\n",
    "# CREATE TABLE nutritionitems (id bigserial PRIMARY KEY,chunk VARCHAR(2500) NOT NULL,embedding vector(768));\n",
    "# CREATE INDEX ON nutritionitems USING hnsw (embedding vector_cosine_ops);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b018ce0f-4e34-4231-9bc7-9458fe31899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "def get_embeddings(recursive_chunks : list) -> list:    \n",
    "    embedding_model = SentenceTransformer(model_name_or_path='all-mpnet-base-v2')\n",
    "    embeddings = embedding_model.encode(recursive_chunks)\n",
    "    embeddings_list = embeddings.tolist()\n",
    "    return embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b212f923-d236-40ea-938d-40f5376f8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from psycopg2 import Error\n",
    "\n",
    "class PostgreSQLManager:\n",
    "    def __init__(self, db_params):\n",
    "        \"\"\"\n",
    "        Initializes the database connection.\n",
    "        db_params should be a dictionary with keys like 'host', 'database', 'user', 'password', 'port'.\n",
    "        \"\"\"\n",
    "        self.db_params = db_params\n",
    "        self.connection = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(**self.db_params)\n",
    "            self.connection.autocommit = False  # Disable autocommit for explicit transactions\n",
    "            print(\"Database connection established successfully.\")\n",
    "        except Error as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "            self.connection = None\n",
    "\n",
    "    def disconnect(self):\n",
    "        \"\"\"Closes the database connection.\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            print(\"Database connection closed.\")\n",
    "            \n",
    "    def execute_batch(self, insert_query, data_to_insert) -> bool:\n",
    "        try:\n",
    "            with self.connection.cursor() as cursor:\n",
    "                psycopg2.extras.execute_batch(cursor, insert_query, data_to_insert)\n",
    "                self.connection.commit()  # Commit changes for CUD operations\n",
    "                return True\n",
    "        except Error as e:\n",
    "            self.connection.rollback()  # Rollback on error\n",
    "            print(f\"Database operation failed: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def execute_select_count(self, select_query) -> int :\n",
    "        try:\n",
    "            with self.connection.cursor() as cursor:\n",
    "                cursor.execute(select_query)\n",
    "                count_after = cursor.fetchone()[0]            \n",
    "                return count_after\n",
    "        except Error as e:\n",
    "            self.connection.rollback()  # Rollback on error\n",
    "            print(f\"Database operation failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def execute_query(self, query, params=None, fetch_result=False):\n",
    "        \"\"\"Helper method to execute a query and handle transactions.\"\"\"\n",
    "        if not self.connection:\n",
    "            print(\"No database connection. Please connect first.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            with self.connection.cursor() as cursor:\n",
    "                if params:\n",
    "                    cursor.execute(query, params)\n",
    "                else:\n",
    "                    cursor.execute(query)\n",
    "\n",
    "                if fetch_result:\n",
    "                    return cursor.fetchall()\n",
    "                else:\n",
    "                    self.connection.commit()  # Commit changes for CUD operations\n",
    "                    return True\n",
    "        except Error as e:\n",
    "            self.connection.rollback()  # Rollback on error\n",
    "            print(f\"Database operation failed: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c7192b4-ff40-4400-aaec-8e9a34fd6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from psycopg2.extras import execute_batch\n",
    "\n",
    "def save_relevant_chunks(recursive_chunks:list, embeddings_list:list) -> int:\n",
    "    load_dotenv()\n",
    "    DB_NAME = os.getenv(\"DB_NAME\")\n",
    "    DB_USER = os.getenv(\"DB_USER\")\n",
    "    DB_PASS = os.getenv(\"DB_PASS\")\n",
    "    DB_HOST = os.getenv(\"DB_HOST\")\n",
    "    DB_PORT = os.getenv(\"DB_PORT\")\n",
    "    db_params = {\n",
    "            \"host\": DB_HOST,\n",
    "            \"database\": DB_NAME,\n",
    "            \"user\": DB_USER,\n",
    "            \"password\": DB_PASS,\n",
    "            \"port\": DB_PORT\n",
    "        }\n",
    "\n",
    "    crud_manager = PostgreSQLManager(db_params)\n",
    "    crud_manager.connect()\n",
    "\n",
    "    data_to_insert = []\n",
    "    for i in range(len(embeddings_list)):\n",
    "        content = recursive_chunks[i]            \n",
    "        embedding = embeddings_list[i]\n",
    "        data_to_insert.append((content,embedding))\n",
    "\n",
    "    if crud_manager.connection:\n",
    "        table_name = \"nutritionitems\"\n",
    "        insert_sql = f\"INSERT INTO {table_name} (chunk, embedding) VALUES (%s, %s)\"        \n",
    "        crud_manager.execute_batch(insert_sql, data_to_insert)\n",
    "        rows_inserted = crud_manager.execute_select_count(f\"SELECT COUNT(*) FROM {table_name};\")        \n",
    "        crud_manager.disconnect()\n",
    "        return rows_inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a3ab6f-7527-4009-8675-ff2dbc3ae6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf_path = \"HumanNutrition.pdf\"\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "#df.head()\n",
    "df.describe().round()\n",
    "\n",
    "raw_text = extract_text_and_spell_check(pages_and_texts)\n",
    "if local_debug :\n",
    "    print(\"raw_text len\", len(raw_text))\n",
    "recursive_chunks = get_recursive_chunks(raw_text)\n",
    "if local_debug :\n",
    "    print(\"recursive_chunks len\", len(recursive_chunks))\n",
    "embeddings_list = get_embeddings(recursive_chunks)\n",
    "if local_debug :\n",
    "    print(\"embeddings_list len\", len(embeddings_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30498cfe-71f5-432d-ae12-a3e421477a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection established successfully.\n",
      "Database connection closed.\n",
      "inserted_rows:  989\n"
     ]
    }
   ],
   "source": [
    "inserted_rows = save_relevant_chunks(recursive_chunks, embeddings_list)\n",
    "if local_debug :\n",
    "    print(\"inserted_rows: \", inserted_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3a7c8-813a-45b2-983f-3b97aabd24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive chunking \n",
    "# chunk by double new lines \\n\\n\n",
    "# chunk by single new line\n",
    "# chunk by sentence\n",
    "# https://github.com/docling-project/docling    -> handle tables\n",
    "# https://github.com/google/langextract\n",
    "# https://spacy.io/api/sentencizer\n",
    "# used this https://huggingface.co/sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2b54b-c394-4b84-abb9-c249e937675d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699a6aa-445a-4658-8b36-11814885e8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb24017-e520-4e1b-9dd1-3b1a53818409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdac99-551b-40b2-a4fc-c6f95133ebb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce42f4-da94-4e0d-a50c-233c414afe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9823b9-e997-46d2-8902-819f512d8f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fd5e5-2c83-48aa-bd89-5bf019acafdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81b7b6-4c31-4a00-802a-1051fbc0ae03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2b11a-2e93-4874-abe4-07327337e22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace24bd-884c-4568-84f0-4f374d700f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364d5d9-9c80-41de-8da9-3406667e32e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e10993-3596-449d-bf97-87d3d6fc8285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0115829-eb25-4555-933d-e3c567c51639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fa86f-3ed2-469a-a05a-07314ac3be96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562ed40-1900-40e7-8fbf-ba6541393cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Environment",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
