{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e63300-9167-476a-83fd-195646552601",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Required packages\n",
    "#!pip install pymupdf\n",
    "#!pip install tqdm\n",
    "#!pip install sentence-transformers tensorflow\n",
    "#!pip install tf-keras\n",
    "#!pip install langchain-text-splitters\n",
    "\n",
    "# Embedding model https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "\n",
    "## optional, info only\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install flash-attn --no-build-isolation # failed because no GPU\n",
    "\n",
    "# Semantic chunk: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "# nltk\n",
    "#!pip install upgrade protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a8e477-640c-4cdc-b85f-f092a1c6921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened 'HumanNutrition.pdf'.\n",
      "Number of pages: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "386it [00:02, 144.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1,\n",
       "  'page_char_count': 202,\n",
       "  'page_word_count': 37,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 50.5,\n",
       "  'text': 'Introduction to  Human Nutrition Second Edition Edited on behalf of The Nutrition Society by Michael J Gibney  Susan A Lanham-New  Aedin Cassidy  Hester H Vorster  A John Wiley & Sons, Ltd., Publication'},\n",
       " {'page_number': 3,\n",
       "  'page_char_count': 32,\n",
       "  'page_word_count': 5,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 8.0,\n",
       "  'text': 'Introduction to  Human Nutrition'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pymupdf\n",
    "#import pdb\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    # Other potential text formatting functions can go here.\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict] :\n",
    "    try:\n",
    "        # Open the PDF document\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "    \n",
    "        # Now you can work with the Document object\n",
    "        print(f\"Successfully opened '{pdf_path}'.\")\n",
    "        print(f\"Number of pages: {doc.page_count}\")\n",
    "    \n",
    "        # To access a specific page (e.g., the first page)\n",
    "        #page = doc[13]\n",
    "        #print(f\"Content of page 1 (first 100 characters): {page.get_text()[:100]}...\")\n",
    "        #pdb.set_trace()\n",
    "        pages_and_texts = []\n",
    "        for page_number, page in tqdm(enumerate(doc)):\n",
    "            text = page.get_text()\n",
    "            text = text_formatter(text)\n",
    "            if len(text) != 0:\n",
    "                pages_and_texts.append({\"page_number\": page_number,\n",
    "                                   \"page_char_count\": len(text),\n",
    "                                   \"page_word_count\": len(text.split(\" \")),\n",
    "                                   \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                   \"page_token_count\": len(text)/4, # 1 token =~4 char\n",
    "                                   \"text\": text })\n",
    "            \n",
    "        # Close the document when you are done\n",
    "        #doc.close()\n",
    "        return pages_and_texts    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{pdf_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "pdf_path = \"HumanNutrition.pdf\"\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042e74d0-f35e-4b65-98ca-b0f17eef9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291bd620-17f4-4c2d-9b45-7da15e4f0057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>202</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>50.50</td>\n",
       "      <td>Introduction to  Human Nutrition Second Editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Introduction to  Human Nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2251</td>\n",
       "      <td>301</td>\n",
       "      <td>1</td>\n",
       "      <td>562.75</td>\n",
       "      <td>The Nutrition Society Textbook Series Introduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>202</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>50.50</td>\n",
       "      <td>Introduction to  Human Nutrition Second Editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2775</td>\n",
       "      <td>458</td>\n",
       "      <td>37</td>\n",
       "      <td>693.75</td>\n",
       "      <td>This edition ﬁ rst published 2009 First editio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            1              202               37                        1   \n",
       "1            3               32                5                        1   \n",
       "2            4             2251              301                        1   \n",
       "3            5              202               37                        1   \n",
       "4            6             2775              458                       37   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0             50.50  Introduction to  Human Nutrition Second Editio...  \n",
       "1              8.00                   Introduction to  Human Nutrition  \n",
       "2            562.75  The Nutrition Society Textbook Series Introduc...  \n",
       "3             50.50  Introduction to  Human Nutrition Second Editio...  \n",
       "4            693.75  This edition ﬁ rst published 2009 First editio...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0ec83b-7ed4-4c47-8ecc-5fe8d87f599f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>381.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>381.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>194.0</td>\n",
       "      <td>3851.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>110.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>99.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>802.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>194.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1064.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>289.0</td>\n",
       "      <td>4673.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>385.0</td>\n",
       "      <td>5313.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1328.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count        381.0            381.0            381.0                    381.0   \n",
       "mean         194.0           3851.0            653.0                     23.0   \n",
       "std          110.0           1090.0            186.0                     12.0   \n",
       "min            1.0             32.0              5.0                      1.0   \n",
       "25%           99.0           3210.0            551.0                     15.0   \n",
       "50%          194.0           4254.0            704.0                     23.0   \n",
       "75%          289.0           4673.0            791.0                     28.0   \n",
       "max          385.0           5313.0            948.0                     91.0   \n",
       "\n",
       "       page_token_count  \n",
       "count             381.0  \n",
       "mean              963.0  \n",
       "std               272.0  \n",
       "min                 8.0  \n",
       "25%               802.0  \n",
       "50%              1064.0  \n",
       "75%              1168.0  \n",
       "max              1328.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2868df5f-bb67-4bad-ac54-a214831c3798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text: ['Introduction to  Human Nutrition Second Edition Edited on behalf of The Nutrition Society by Michael J Gibney  Susan A Lanham-New  Aedin Cassidy  Hester H Vorster  A John Wiley & Sons, Ltd., Publication', 'Introduction to  Human Nutrition']\n"
     ]
    }
   ],
   "source": [
    "raw_text = []\n",
    "for item in pages_and_texts:\n",
    "    raw_text.append(item[\"text\"])\n",
    "print(f\"Extracted text: {raw_text[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91cb9c7-c217-4280-bfd6-97ea19e01496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anila\\ml\\mlenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anila\\ml\\mlenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "No of recursive chunks 4207\n",
      "[Document(metadata={}, page_content='Introduction to  Human Nutrition Second Edition Edited on behalf of The Nutrition Society by Michael J Gibney  Susan A Lanham-New  Aedin Cassidy  Hester H Vorster  A John Wiley & Sons, Ltd., Publication'), Document(metadata={}, page_content='Introduction to  Human Nutrition')]\n",
      "Recursive chunks: ------  ['Introduction to  Human Nutrition Second Edition Edited on behalf of The Nutrition Society by Michael J Gibney  Susan A Lanham-New  Aedin Cassidy  Hester H Vorster  A John Wiley & Sons, Ltd., Publication', 'Introduction to  Human Nutrition']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "# chunk_size: The maximum size of each chunk (in characters by default).\n",
    "# chunk_overlap: The number of characters to overlap between consecutive chunks.\n",
    "# separators: A list of characters to try splitting by, in order of preference.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=383,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try splitting by paragraphs, then newlines, then spaces, then characters\n",
    ")\n",
    "\n",
    "# Split the text into documents\n",
    "#docs = text_splitter.create_documents([text])\n",
    "docs = text_splitter.create_documents(raw_text)\n",
    "#pages_and_texts\n",
    "print(\"No of recursive chunks\", len(docs))\n",
    "# Print the resulting chunks\n",
    "print(docs[:2])\n",
    "recursive_chunks = []\n",
    "for i, doc in enumerate(docs):\n",
    "    recursive_chunks.append(doc.page_content)\n",
    "    #print(f\"Chunk {i+1}:\\n{doc.page_content}\\n---\")\n",
    "\n",
    "print(\"Recursive chunks: ------ \", recursive_chunks[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e017ae98-d132-40be-a241-e45e7f488b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT id,SUBSTRING(ni.chunk, 1, 200) AS short_chunk, embedding as dimension FROM nutritionitems ni LIMIT 5;\n",
    "# SELECT id,SUBSTRING(ni.chunk, 1, 200) AS short_chunk, embedding as dimension FROM nutritionitems ni\n",
    "# ORDER BY embedding <=> '[5.99734336e-02,-1.30569497e-02]'\n",
    "# LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b018ce0f-4e34-4231-9bc7-9458fe31899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = recursive_chunks\n",
    "embedding_model = SentenceTransformer(model_name_or_path='all-mpnet-base-v2')\n",
    "embeddings = embedding_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8216150b-cac3-48ec-8bd2-30925b6c5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chunks 4207\n",
      "total embeddings 4207\n",
      "length of each embedding 768\n",
      "embeddings type <class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"total chunks\",len(recursive_chunks))\n",
    "print(\"total embeddings\",len(embeddings))\n",
    "print(\"length of each embedding\",len(embeddings[0]))\n",
    "print(\"embeddings type\",type(embeddings))\n",
    "embeddings_list = embeddings.tolist()\n",
    "print(type(embeddings_list))\n",
    "\n",
    "# drop table nutritionitems;\n",
    "# CREATE TABLE nutritionitems (id bigserial PRIMARY KEY,chunk VARCHAR(400) NOT NULL,embedding vector(768));\n",
    "# CREATE INDEX ON nutritionitems USING hnsw (embedding vector_cosine_ops);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b212f923-d236-40ea-938d-40f5376f8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def connect_to_db():\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    DB_NAME = os.getenv(\"DB_NAME\")\n",
    "    DB_USER = os.getenv(\"DB_USER\")\n",
    "    DB_PASS = os.getenv(\"DB_PASS\")\n",
    "    DB_HOST = os.getenv(\"DB_HOST\")\n",
    "    DB_PORT = os.getenv(\"DB_PORT\")\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT\n",
    "        )\n",
    "        print(\"Connected to PostgreSQL database successfully!\")\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL database: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07a3ab6f-7527-4009-8675-ff2dbc3ae6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL database successfully!\n",
      "Database connected successfully!\n"
     ]
    }
   ],
   "source": [
    "conn = connect_to_db() \n",
    "\n",
    "cur = conn.cursor()\n",
    "for i in range(len(embeddings_list)):\n",
    "    embedding = embeddings_list[i]\n",
    "    content = recursive_chunks[i]    \n",
    "    cur.execute(\"INSERT INTO nutritionitems (chunk, embedding) VALUES (%s, %s)\",(content, embedding))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"loaded embeddings to nutritionitems table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3a7c8-813a-45b2-983f-3b97aabd24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive chunking \n",
    "# chunk by double new lines \\n\\n\n",
    "# chunk by single new line\n",
    "# chunk by sentence\n",
    "# https://github.com/docling-project/docling    -> handle tables\n",
    "# https://github.com/google/langextract\n",
    "# https://spacy.io/api/sentencizer\n",
    "# used this https://huggingface.co/sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2b54b-c394-4b84-abb9-c249e937675d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699a6aa-445a-4658-8b36-11814885e8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb24017-e520-4e1b-9dd1-3b1a53818409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdac99-551b-40b2-a4fc-c6f95133ebb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce42f4-da94-4e0d-a50c-233c414afe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9823b9-e997-46d2-8902-819f512d8f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fd5e5-2c83-48aa-bd89-5bf019acafdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81b7b6-4c31-4a00-802a-1051fbc0ae03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2b11a-2e93-4874-abe4-07327337e22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace24bd-884c-4568-84f0-4f374d700f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364d5d9-9c80-41de-8da9-3406667e32e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e10993-3596-449d-bf97-87d3d6fc8285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0115829-eb25-4555-933d-e3c567c51639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fa86f-3ed2-469a-a05a-07314ac3be96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562ed40-1900-40e7-8fbf-ba6541393cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Environment",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
