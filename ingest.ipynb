{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93e63300-9167-476a-83fd-195646552601",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Required packages\n",
    "#!pip install pymupdf\n",
    "#!pip install tqdm\n",
    "#!pip install sentence-transformers tensorflow\n",
    "#!pip install tf-keras\n",
    "#!pip install langchain-text-splitters\n",
    "\n",
    "# Embedding model https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "\n",
    "## optional, info only\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install flash-attn --no-build-isolation # failed because no GPU\n",
    "\n",
    "# Semantic chunk: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "# nltk\n",
    "#!pip install upgrade protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48a8e477-640c-4cdc-b85f-f092a1c6921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened 'HumanNutrition.pdf'.\n",
      "Number of pages: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "386it [00:05, 70.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pymupdf\n",
    "import re\n",
    "\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    para_end = \"<paraend>\"\n",
    "    word_cut = \"<wordcut>\"\n",
    "    words_to_clean = {\n",
    "        \"ﬁ \":\"fi\",\n",
    "        \"ﬂ \":\"fl\",\n",
    "        \".\\n\\n\\n\": para_end,\n",
    "        \".\\n\\n\": para_end,\n",
    "        \"\\n\\n\": para_end,\n",
    "        \":\\n\": para_end,\n",
    "        \".\\n\": para_end,\n",
    "        \"-\\n\": word_cut,\n",
    "        \"\\n\": \" \",\n",
    "        para_end : \".\\n\",\n",
    "        word_cut :\"\",\n",
    "    }\n",
    "\n",
    "    cleaned_text = text\n",
    "    for invalid_word, valid_word in words_to_clean.items():\n",
    "        cleaned_text = cleaned_text.replace(invalid_word, valid_word)        \n",
    "    \n",
    "    # cleaned_text = cleaned_text.replace(\".\\n\\n\\n\", \"<paraend>\")\n",
    "    # cleaned_text = cleaned_text.replace(\".\\n\\n\", \"<paraend>\")\n",
    "    # cleaned_text = cleaned_text.replace(\"\\n\\n\", \"<paraend>\")\n",
    "    # cleaned_text = cleaned_text.replace(\":\\n\", \"<paraend>\")\n",
    "    # cleaned_text = cleaned_text.replace(\".\\n\", \"<paraend>\")\n",
    "    # cleaned_text = cleaned_text.replace(\"-\\n\", \"<wordcut>\")\n",
    "    # cleaned_text = cleaned_text.replace(\"\\n\", \" \")\n",
    "    # cleaned_text = cleaned_text.replace(\"<paraend>\", \".\\n \")\n",
    "    # cleaned_text = cleaned_text.replace(\"<wordcut>\",\"\")\n",
    "    \n",
    "    # Remove excessive spaces and replace with a single space    \n",
    "    cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict] :\n",
    "    try:\n",
    "        # Open the PDF document\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "    \n",
    "        # Now you can work with the Document object\n",
    "        print(f\"Successfully opened '{pdf_path}'.\")\n",
    "        print(f\"Number of pages: {doc.page_count}\")\n",
    "    \n",
    "        # To access a specific page (e.g., the first page)\n",
    "        #page = doc[13]\n",
    "        #print(f\"Content of page 1 (first 100 characters): {page.get_text()[:100]}...\")\n",
    "        #pdb.set_trace()\n",
    "        header_height = 50 # Adjust as needed\n",
    "        footer_height = 50 # Adjust as needed\n",
    "        pages_and_texts = []\n",
    "        for page_number, page in tqdm(enumerate(doc)):\n",
    "            if page_number > 0 :\n",
    "                page_rect = page.rect\n",
    "                clip = pymupdf.Rect(\n",
    "                    page_rect.x0,\n",
    "                    page_rect.y0 + header_height,\n",
    "                    page_rect.x1,\n",
    "                    page_rect.y1 - footer_height\n",
    "                )\n",
    "                #get_text(option, *, clip=None, flags=None, textpage=None, sort=False, delimiters=None)\n",
    "                #flags = pymupdf.TEXT_INHIBIT_SPACES            \n",
    "                text = page.get_text(clip=clip)\n",
    "                text = text_formatter(text)          \n",
    "                \n",
    "                if len(text) != 0 and page_number > 14 :\n",
    "                    pages_and_texts.append({\"page_number\": page_number-14,\n",
    "                                       \"page_char_count\": len(text),\n",
    "                                       \"page_word_count\": len(text.split(\" \")),\n",
    "                                       \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                       \"page_token_count\": len(text)/4, # 1 token =~4 char\n",
    "                                       \"text\": text })\n",
    "            \n",
    "        # Close the document when you are done\n",
    "        doc.close()\n",
    "        return pages_and_texts    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{pdf_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "pdf_path = \"HumanNutrition.pdf\"\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "291bd620-17f4-4c2d-9b45-7da15e4f0057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>370.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>186.0</td>\n",
       "      <td>3776.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>944.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>107.0</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>93.0</td>\n",
       "      <td>3159.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>186.0</td>\n",
       "      <td>4160.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>278.0</td>\n",
       "      <td>4554.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>371.0</td>\n",
       "      <td>5180.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1295.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count        370.0            370.0            370.0                    370.0   \n",
       "mean         186.0           3776.0            569.0                     17.0   \n",
       "std          107.0           1039.0            162.0                      9.0   \n",
       "min            1.0            233.0             37.0                      1.0   \n",
       "25%           93.0           3159.0            469.0                     11.0   \n",
       "50%          186.0           4160.0            618.0                     17.0   \n",
       "75%          278.0           4554.0            688.0                     22.0   \n",
       "max          371.0           5180.0            835.0                     65.0   \n",
       "\n",
       "       page_token_count  \n",
       "count             370.0  \n",
       "mean              944.0  \n",
       "std               260.0  \n",
       "min                58.0  \n",
       "25%               790.0  \n",
       "50%              1040.0  \n",
       "75%              1138.0  \n",
       "max              1295.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()\n",
    "df.describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2868df5f-bb67-4bad-ac54-a214831c3798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text appended to 'misspelled_words.txt'.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import os\n",
    "\n",
    "misspelled_words = []\n",
    "raw_text = []\n",
    "\n",
    "misspell_file = \"misspelled_words_test.txt\"\n",
    "\n",
    "if os.path.exists(misspell_file):\n",
    "    os.remove(misspell_file)\n",
    "\n",
    "with open(misspell_file, 'w', encoding='utf-8') as file:\n",
    "    for item in pages_and_texts :   \n",
    "        raw_text.append(item[\"text\"])\n",
    "        cleaned_text = re.sub(r'[^\\w\\s]', '', item[\"text\"])\n",
    "        spell = SpellChecker()    \n",
    "        misspelled = spell.unknown(cleaned_text.split())    \n",
    "        for word in misspelled:\n",
    "            misspelled_words.append(word)\n",
    "            file.write(word + \"\\n\")     \n",
    "\n",
    "print(f\"Text appended to '{file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e91cb9c7-c217-4280-bfd6-97ea19e01496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of recursive chunks 989\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "# chunk_size: The maximum size of each chunk (in characters by default).\n",
    "# chunk_overlap: The number of characters to overlap between consecutive chunks.\n",
    "# separators: A list of characters to try splitting by, in order of preference.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=40,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try splitting by paragraphs, then newlines, then spaces, then characters\n",
    ")\n",
    "\n",
    "# Split the text into documents\n",
    "#docs = text_splitter.create_documents([text])\n",
    "docs = text_splitter.create_documents(raw_text)\n",
    "#pages_and_texts\n",
    "print(\"No of recursive chunks\", len(docs))\n",
    "# Print the resulting chunks\n",
    "\n",
    "recursive_chunks = []\n",
    "chunks_file = \"chunks_test.txt\"\n",
    "\n",
    "if os.path.exists(chunks_file):\n",
    "    os.remove(chunks_file)\n",
    "\n",
    "with open(chunks_file, 'w', encoding='utf-8') as file:\n",
    "    for i, doc in enumerate(docs):\n",
    "        recursive_chunks.append(doc.page_content)\n",
    "        file.write(doc.page_content + \"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e017ae98-d132-40be-a241-e45e7f488b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT id,SUBSTRING(ni.chunk, 1, 200) AS short_chunk, embedding as dimension FROM nutritionitems ni LIMIT 5;\n",
    "# SELECT id,SUBSTRING(ni.chunk, 1, 200) AS short_chunk, embedding as dimension FROM nutritionitems ni\n",
    "# ORDER BY embedding <=> '[5.99734336e-02,-1.30569497e-02]'\n",
    "# LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b018ce0f-4e34-4231-9bc7-9458fe31899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = recursive_chunks\n",
    "embedding_model = SentenceTransformer(model_name_or_path='all-mpnet-base-v2')\n",
    "embeddings = embedding_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8216150b-cac3-48ec-8bd2-30925b6c5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chunks 989\n",
      "total embeddings 989\n",
      "length of each embedding 768\n",
      "embeddings type <class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"total chunks\",len(recursive_chunks))\n",
    "print(\"total embeddings\",len(embeddings))\n",
    "print(\"length of each embedding\",len(embeddings[0]))\n",
    "print(\"embeddings type\",type(embeddings))\n",
    "embeddings_list = embeddings.tolist()\n",
    "print(type(embeddings_list))\n",
    "\n",
    "# drop table nutritionitems;\n",
    "# CREATE TABLE nutritionitems (id bigserial PRIMARY KEY,chunk VARCHAR(2500) NOT NULL,embedding vector(768));\n",
    "# CREATE INDEX ON nutritionitems USING hnsw (embedding vector_cosine_ops);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b212f923-d236-40ea-938d-40f5376f8bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07a3ab6f-7527-4009-8675-ff2dbc3ae6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL database successfully!\n",
      "Rows after insert into nutritionitems table: 989\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def connect_to_db():\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    DB_NAME = os.getenv(\"DB_NAME\")\n",
    "    DB_USER = os.getenv(\"DB_USER\")\n",
    "    DB_PASS = os.getenv(\"DB_PASS\")\n",
    "    DB_HOST = os.getenv(\"DB_HOST\")\n",
    "    DB_PORT = os.getenv(\"DB_PORT\")\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT\n",
    "        )\n",
    "        print(\"Connected to PostgreSQL database successfully!\")\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL database: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "conn = connect_to_db() \n",
    "table_name = \"nutritionitems\"\n",
    "cur = conn.cursor()\n",
    "for i in range(len(embeddings_list)):\n",
    "    embedding = embeddings_list[i]\n",
    "    content = recursive_chunks[i]    \n",
    "    cur.execute(f\"INSERT INTO {table_name} (chunk, embedding) VALUES (%s, %s)\",(content, embedding))\n",
    "    \n",
    "conn.commit()\n",
    "cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "count_after = cur.fetchone()[0]\n",
    "print(f\"Rows after insert into {table_name} table: {count_after}\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3a7c8-813a-45b2-983f-3b97aabd24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive chunking \n",
    "# chunk by double new lines \\n\\n\n",
    "# chunk by single new line\n",
    "# chunk by sentence\n",
    "# https://github.com/docling-project/docling    -> handle tables\n",
    "# https://github.com/google/langextract\n",
    "# https://spacy.io/api/sentencizer\n",
    "# used this https://huggingface.co/sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2b54b-c394-4b84-abb9-c249e937675d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699a6aa-445a-4658-8b36-11814885e8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb24017-e520-4e1b-9dd1-3b1a53818409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdac99-551b-40b2-a4fc-c6f95133ebb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce42f4-da94-4e0d-a50c-233c414afe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9823b9-e997-46d2-8902-819f512d8f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fd5e5-2c83-48aa-bd89-5bf019acafdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81b7b6-4c31-4a00-802a-1051fbc0ae03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2b11a-2e93-4874-abe4-07327337e22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace24bd-884c-4568-84f0-4f374d700f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364d5d9-9c80-41de-8da9-3406667e32e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e10993-3596-449d-bf97-87d3d6fc8285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0115829-eb25-4555-933d-e3c567c51639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fa86f-3ed2-469a-a05a-07314ac3be96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562ed40-1900-40e7-8fbf-ba6541393cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Environment",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
