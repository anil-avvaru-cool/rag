{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0775880-2458-48ff-93b9-0fb6795fe984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTEB: Massive text embedding model \n",
    "# https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "# open source llm https://huggingface.co/google/gemma-7b-it\n",
    "#create write token from huggingface\n",
    "# gemma-7b-it, gemma-2b-it\n",
    "import os\n",
    "# Flag to print debug messages/write debug files\n",
    "local_debug = True\n",
    "# Remove huggingface token after execution for security purpose and comment below line\n",
    "#os.environ['HF_TOKEN'] = '<token>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c10205-a58c-4a45-868d-3b97bddef701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anila\\ml\\mlenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anila\\ml\\mlenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_query_embedding(input_query: str) -> list:\n",
    "    embedding_model = SentenceTransformer(model_name_or_path='all-mpnet-base-v2')    \n",
    "    query_embedding = embedding_model.encode(input_query)\n",
    "    query_embedding_list = query_embedding.tolist()        \n",
    "    return query_embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7d783f-ff77-4695-9a41-79d06393adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CREATE OR REPLACE FUNCTION get_embedding(\t\n",
    "# \trows_limit int,\n",
    "#     embedding_input vector(768)\n",
    "# )\n",
    "# RETURNS TABLE (id bigint, chunk varchar(2500), embedding vector(768), cosine_similarity DOUBLE PRECISION)\n",
    "# LANGUAGE plpgsql\n",
    "# AS $$\n",
    "# BEGIN    \n",
    "# \tRETURN QUERY SELECT ni.id,ni.chunk, ni.embedding, 1 - (ni.embedding <=> embedding_input) AS cosine_similarity FROM nutritionitems ni\n",
    "# \tORDER BY cosine_similarity DESC LIMIT rows_limit;\n",
    "# END;\n",
    "# $$;\n",
    "\n",
    "# select get_embedding(2::int,'[5.99734336e-02,-1.30569497e-02]'::vector);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa50d5ce-331c-470c-903c-cf3714dec8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from psycopg2 import Error\n",
    "\n",
    "class PostgreSQLManager:\n",
    "    def __init__(self, db_params):\n",
    "        \"\"\"\n",
    "        Initializes the database connection.\n",
    "        db_params should be a dictionary with keys like 'host', 'database', 'user', 'password', 'port'.\n",
    "        \"\"\"\n",
    "        self.db_params = db_params\n",
    "        self.connection = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(**self.db_params)\n",
    "            self.connection.autocommit = False  # Disable autocommit for explicit transactions\n",
    "            print(\"Database connection established successfully.\")\n",
    "        except Error as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "            self.connection = None\n",
    "\n",
    "    def disconnect(self):\n",
    "        \"\"\"Closes the database connection.\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "    def execute_query(self, query, params=None, fetch_result=False):\n",
    "        \"\"\"Helper method to execute a query and handle transactions.\"\"\"\n",
    "        if not self.connection:\n",
    "            print(\"No database connection. Please connect first.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            with self.connection.cursor() as cursor:\n",
    "                if params:\n",
    "                    cursor.execute(query, params)\n",
    "                else:\n",
    "                    cursor.execute(query)\n",
    "\n",
    "                if fetch_result:\n",
    "                    return cursor.fetchall()\n",
    "                else:\n",
    "                    self.connection.commit()  # Commit changes for CUD operations\n",
    "                    return True\n",
    "        except Error as e:\n",
    "            self.connection.rollback()  # Rollback on error\n",
    "            print(f\"Database operation failed: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f68d548-466d-4ea2-8aa0-ba3c18dfea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def get_relevant_chunks(limit:int, query_embedding_list:str) -> list:\n",
    "    load_dotenv()\n",
    "    DB_NAME = os.getenv(\"DB_NAME\")\n",
    "    DB_USER = os.getenv(\"DB_USER\")\n",
    "    DB_PASS = os.getenv(\"DB_PASS\")\n",
    "    DB_HOST = os.getenv(\"DB_HOST\")\n",
    "    DB_PORT = os.getenv(\"DB_PORT\")\n",
    "    db_params = {\n",
    "            \"host\": DB_HOST,\n",
    "            \"database\": DB_NAME,\n",
    "            \"user\": DB_USER,\n",
    "            \"password\": DB_PASS,\n",
    "            \"port\": DB_PORT\n",
    "        }\n",
    "\n",
    "    crud_manager = PostgreSQLManager(db_params)\n",
    "    crud_manager.connect()\n",
    "\n",
    "    if crud_manager.connection:\n",
    "        select_sql = \"\"\"SELECT * from get_embedding(%s,%s::vector);\"\"\"\n",
    "        nutritionitems = crud_manager.execute_query(select_sql, (limit,query_embedding_list), True)\n",
    "        print(\"nutritionitems len\", len(nutritionitems))\n",
    "        retrieved_chunks_dict = []\n",
    "        retrieved_chunks = []\n",
    "        for item in nutritionitems:\n",
    "            retrieved_chunks.append(item[1])\n",
    "            retrieved_chunks_dict.append({\"id\":item[0],\n",
    "                                     \"chunk\": item[1],\n",
    "                                     \"cosine_similarity\": item[3]\n",
    "                                    })\n",
    "        crud_manager.disconnect()\n",
    "        if local_debug:\n",
    "            retrieved_chunks_file = \"Retrieved_chunks_test.txt\"        \n",
    "            if os.path.exists(retrieved_chunks_file):\n",
    "                os.remove(retrieved_chunks_file)        \n",
    "            \n",
    "            with open(retrieved_chunks_file, 'w', encoding='utf-8') as file:\n",
    "                for item in retrieved_chunks_dict :  \n",
    "                    file.write(f\"{item[\"id\"]} | {item[\"cosine_similarity\"]} | {item[\"chunk\"]} \\n\\n\")\n",
    "        return retrieved_chunks\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40401df-ca48-4d4d-b04b-045e74a99b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query:str,\n",
    "                    context_items: list[dict])-> str:\n",
    "    context = \"- \"+\"\\n- \".join([item for item in context_items])            \n",
    "    \n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query\n",
    "Don't return the thinking, only return answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What are fat-soluble vitamins?\n",
    "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and vitamin K. These vitamins dissolve in fat and are absorbed and stored in the body's fatty tissues\n",
    "\\nExample 2:\n",
    "Query: What are the causes of type 2 diabetes?\n",
    "Answer: Insulin Resistance – Body’s cells don’t respond properly to insulin, leading to elevated blood glucose. Pancreatic Dysfunction – Over time, the pancreas can’t produce enough insulin to compensate for resistance.\n",
    "\\nNow use the following context items to answer user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f14efe-e936-4fcf-970c-98d3a527079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "def connect_to_huggingface():\n",
    "    hf_token_val = os.getenv('HF_TOKEN')\n",
    "    login(token=hf_token_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d83b41-2fc7-413c-a8ce-0cf819958831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def chat_template(formatted_prompt: str) -> str:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "    # Define a chat history\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": formatted_prompt}    \n",
    "    ]    \n",
    "    # Apply the chat template, which automatically handles BOS/EOS and formatting\n",
    "    chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    prompt_file = \"prompt_test.txt\"    \n",
    "    if os.path.exists(prompt_file):\n",
    "        os.remove(prompt_file)    \n",
    "    with open(prompt_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(chat_prompt)\n",
    "    return chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e21c41-0a4c-4892-b961-8edbc614add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate(chat_prompt: str) -> str:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2b-it\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )    \n",
    "    inputs = tokenizer.encode(chat_prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=250)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc73383b-8e03-427c-9fb8-73449f59334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding len 768\n",
      "Database connection established successfully.\n",
      "nutritionitems len 5\n",
      "Database connection closed.\n",
      "retrieved_chunks len 5\n",
      "formatted_prompt len 9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_prompt len 9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.46it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_debug :\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mchat_prompt len\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chat_prompt))\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m llm_output = \u001b[43mllm_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mllm_output: -----\u001b[39m\u001b[33m\"\u001b[39m, llm_output)    \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mllm_generate\u001b[39m\u001b[34m(chat_prompt)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm_generate\u001b[39m(chat_prompt: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      2\u001b[39m     model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgoogle/gemma-2b-it\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m         torch_dtype=torch.bfloat16\n\u001b[32m      5\u001b[39m     )    \n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     inputs = \u001b[43mtokenizer\u001b[49m.encode(chat_prompt, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=\u001b[32m250\u001b[39m)\n\u001b[32m      8\u001b[39m     output = tokenizer.decode(outputs[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# No Answer\n",
    "#input_query = \"Describe the process of digestion and absorption of nutrients in the human body.\"\n",
    "#input_query = \"What is Metabolism of Proteins?\"\n",
    "input_query = \"Explain Digestion and Metabolism of Carbohydrates\"\n",
    "query_embedding_list = get_query_embedding(input_query)\n",
    "if local_debug :    \n",
    "    print(\"query_embedding len\", len(query_embedding_list))\n",
    "\n",
    "retrieved_chunks = get_relevant_chunks(5, query_embedding_list)\n",
    "if local_debug :\n",
    "    print(\"retrieved_chunks len\", len(retrieved_chunks))\n",
    "\n",
    "formatted_prompt = prompt_formatter(input_query,retrieved_chunks)\n",
    "if local_debug :\n",
    "    print(\"formatted_prompt len\", len(formatted_prompt))\n",
    "connect_to_huggingface()\n",
    "\n",
    "chat_prompt = chat_template(formatted_prompt)\n",
    "if local_debug :\n",
    "    print(\"chat_prompt len\", len(chat_prompt))\n",
    "llm_output = llm_generate(chat_prompt)\n",
    "print(\"llm_output: -----\", llm_output)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3510d-4748-466c-a339-493a6e0ba76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088d79a-f027-47fe-8b49-5e0983ac746f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b631b77-aa78-47f7-8b5b-ac3b654640a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567151b-14c2-40e4-8d07-d6530e5a761a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3eb994-3c1e-40e6-b5e5-9481683765f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bab488-4af1-4579-8987-e9ffd18ca781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833450f2-71d2-4242-8b71-1b2047342a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595760fe-41dc-4160-99b6-a9192659b51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80489c5c-5f6d-41ec-bf30-ca87d68a9e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd6454f-3c76-4f74-bc98-76c567fa65fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0679b0-ec77-45cc-a9b9-a844672b589f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Environment",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
