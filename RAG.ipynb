{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0775880-2458-48ff-93b9-0fb6795fe984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTEB: Massive text embedding model \n",
    "# https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "# open source llm https://huggingface.co/google/gemma-7b-it\n",
    "#create write token from huggingface\n",
    "# gemma-7b-it \n",
    "# gemma-2b-it\n",
    "\n",
    "# Flag to print debug messages/write debug files\n",
    "local_debug = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "24c10205-a58c-4a45-868d-3b97bddef701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_query_embedding(input_query: str) -> list:\n",
    "    embedding_model = SentenceTransformer(model_name_or_path='all-mpnet-base-v2')    \n",
    "    query_embedding = embedding_model.encode(input_query)\n",
    "    query_embedding_list = query_embedding.tolist()        \n",
    "    return query_embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a7d783f-ff77-4695-9a41-79d06393adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CREATE OR REPLACE FUNCTION get_embedding(\t\n",
    "# \trows_limit int,\n",
    "#     embedding_input vector(768)\n",
    "# )\n",
    "# RETURNS TABLE (id bigint, chunk varchar(2500), embedding vector(768), cosine_similarity DOUBLE PRECISION)\n",
    "# LANGUAGE plpgsql\n",
    "# AS $$\n",
    "# BEGIN    \n",
    "# \tRETURN QUERY SELECT ni.id,ni.chunk, ni.embedding, 1 - (ni.embedding <=> embedding_input) AS cosine_similarity FROM nutritionitems ni\n",
    "# \tORDER BY cosine_similarity DESC LIMIT rows_limit;\n",
    "# END;\n",
    "# $$;\n",
    "\n",
    "# select get_embedding(2::int,'[5.99734336e-02,-1.30569497e-02]'::vector);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f299d889-5555-4162-af56-95e6865ede12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def connect_to_db():\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    DB_NAME = os.getenv(\"DB_NAME\")\n",
    "    DB_USER = os.getenv(\"DB_USER\")\n",
    "    DB_PASS = os.getenv(\"DB_PASS\")\n",
    "    DB_HOST = os.getenv(\"DB_HOST\")\n",
    "    DB_PORT = os.getenv(\"DB_PORT\")\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT\n",
    "        )\n",
    "        if local_debug:            \n",
    "            print(\"Connected to PostgreSQL database successfully!\")\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL database: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_data(conn):    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        select_sql = \"\"\"SELECT * from get_embedding(%s,%s::vector);\"\"\"\n",
    "        cur.execute(select_sql, (5, query_embedding_list))\n",
    "        nutritionitems = cur.fetchall()\n",
    "        #print(\"nutritionitems length\", nutritionitems)\n",
    "        retrieved_chunks_dict = []\n",
    "        retrieved_chunks = []\n",
    "        for item in nutritionitems:\n",
    "            retrieved_chunks.append(item[1])\n",
    "            retrieved_chunks_dict.append({\"id\":item[0],\n",
    "                                     \"chunk\": item[1],\n",
    "                                     \"cosine_similarity\": item[3]\n",
    "                                    })            \n",
    "        cur.close()\n",
    "        if conn and local_debug:\n",
    "            conn.close()\n",
    "            print(\"Connection closed.\")\n",
    "        if local_debug:\n",
    "            retrieved_chunks_file = \"Retrieved_chunks_test.txt\"        \n",
    "            if os.path.exists(retrieved_chunks_file):\n",
    "                os.remove(retrieved_chunks_file)        \n",
    "            \n",
    "            with open(retrieved_chunks_file, 'w', encoding='utf-8') as file:\n",
    "                for item in retrieved_chunks_dict :  \n",
    "                    file.write(f\"{item[\"id\"]} | {item[\"cosine_similarity\"]} | {item[\"chunk\"]} \\n\\n\")\n",
    "        return retrieved_chunks\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error fetching data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c40401df-ca48-4d4d-b04b-045e74a99b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query:str,\n",
    "                    context_items: list[dict])-> str:\n",
    "    context = \"- \"+\"\\n- \".join([item for item in context_items])            \n",
    "    \n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query\n",
    "Don't return the thinking, only return answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What are fat-soluble vitamins?\n",
    "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and vitamin K. These vitamins dissolve in fat and are absorbed and stored in the body's fatty tissues\n",
    "\\nExample 2:\n",
    "Query: What are the causes of type 2 diabetes?\n",
    "Answer: Insulin Resistance – Body’s cells don’t respond properly to insulin, leading to elevated blood glucose. Pancreatic Dysfunction – Over time, the pancreas can’t produce enough insulin to compensate for resistance.\n",
    "\\nNow use the following context items to answer user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4237eca7-1353-4d57-8de5-d4d6d758665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt_formatter(input_query,retrieved_chunks)\n",
    "#print(\"formatted_prompt\",formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48f14efe-e936-4fcf-970c-98d3a527079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# hugging face token stored in env variable HF_TOKEN\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Using os.getenv()\n",
    "# Returns the value of 'MY_VARIABLE' or None if it doesn't exist\n",
    "# os.environ['HF_TOKEN'] = ''\n",
    "hf_token_val = os.getenv('HF_TOKEN') \n",
    "\n",
    "# your_token = \"hf_YOUR_ACTUAL_TOKEN_HERE\" # Replace with your copied token\n",
    "login(token=hf_token_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0d83b41-2fc7-413c-a8ce-0cf819958831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "\n",
    "# Define a chat history\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": formatted_prompt}    \n",
    "]\n",
    "\n",
    "# Apply the chat template, which automatically handles BOS/EOS and formatting\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt_file = \"prompt_test.txt\"\n",
    "\n",
    "if os.path.exists(prompt_file):\n",
    "    os.remove(prompt_file)\n",
    "\n",
    "with open(prompt_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01e21c41-0a4c-4892-b961-8edbc614add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "#input_text = \"Write me a poem about Machine Learning.\"\n",
    "#input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer.encode(formatted_prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=250)\n",
    "\n",
    "#outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc73383b-8e03-427c-9fb8-73449f59334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding len 768\n",
      "Connected to PostgreSQL database successfully!\n",
      "Connection closed.\n",
      "retrieved_chunks len 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# No Answer\n",
    "#input_query = \"Describe the process of digestion and absorption of nutrients in the human body.\"\n",
    "input_query = \"What is Metabolism of Proteins?\"\n",
    "query_embedding_list = get_query_embedding(input_query)\n",
    "if local_debug :    \n",
    "    print(\"query_embedding len\", len(query_embedding_list))\n",
    "\n",
    "conn = connect_to_db()\n",
    "retrieved_chunks = fetch_data(conn)\n",
    "if local_debug :\n",
    "    print(\"retrieved_chunks len\", len(retrieved_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3510d-4748-466c-a339-493a6e0ba76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088d79a-f027-47fe-8b49-5e0983ac746f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b631b77-aa78-47f7-8b5b-ac3b654640a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567151b-14c2-40e4-8d07-d6530e5a761a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3eb994-3c1e-40e6-b5e5-9481683765f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bab488-4af1-4579-8987-e9ffd18ca781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833450f2-71d2-4242-8b71-1b2047342a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595760fe-41dc-4160-99b6-a9192659b51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80489c5c-5f6d-41ec-bf30-ca87d68a9e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd6454f-3c76-4f74-bc98-76c567fa65fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0679b0-ec77-45cc-a9b9-a844672b589f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Environment",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
